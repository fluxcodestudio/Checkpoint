---
phase: 07-performance-optimization
plan: 02
type: execute
---

<objective>
Replace byte-by-byte file comparison with hash-based comparison for faster change detection.

Purpose: Reduce per-file comparison overhead from O(n) byte reads to O(1) hash lookup.
Output: Hash-based file comparison in backup-lib.sh with cached checksums.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-performance-optimization/07-01-SUMMARY.md

@lib/backup-lib.sh
@bin/backup-now.sh

**Current bottleneck (backup-now.sh copy loop):**
```bash
# For each file:
cmp -s "$file" "$current_file"  # Byte-by-byte comparison - SLOW
```

**Problem:** With 500 changed files averaging 5MB each, `cmp` reads both files fully.

**Solution:** Use sha256 hash comparison:
- Store hash in extended attribute or sidecar file
- Compare hashes instead of bytes
- Only recompute hash if mtime changed

**Constraint:** macOS uses shasum, not sha256sum.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add hash-based file comparison functions</name>
  <files>lib/backup-lib.sh</files>
  <action>
Add new functions to backup-lib.sh for hash-based comparison:

```bash
# Get file hash (uses cache if mtime unchanged)
get_file_hash() {
    local file="$1"
    local hash_cache="${BACKUP_DIR}/.hash-cache"
    local file_mtime file_hash cached_mtime cached_hash

    # Get current mtime
    file_mtime=$(stat -f%m "$file" 2>/dev/null) || return 1

    # Check cache
    if [ -f "$hash_cache" ]; then
        cached_line=$(grep "^${file}|" "$hash_cache" 2>/dev/null || true)
        if [ -n "$cached_line" ]; then
            cached_mtime=$(echo "$cached_line" | cut -d'|' -f2)
            cached_hash=$(echo "$cached_line" | cut -d'|' -f3)
            if [ "$file_mtime" = "$cached_mtime" ]; then
                echo "$cached_hash"
                return 0
            fi
        fi
    fi

    # Compute new hash
    file_hash=$(shasum -a 256 "$file" 2>/dev/null | cut -d' ' -f1) || return 1

    # Update cache (atomic write)
    local tmp_cache=$(mktemp)
    grep -v "^${file}|" "$hash_cache" 2>/dev/null > "$tmp_cache" || true
    echo "${file}|${file_mtime}|${file_hash}" >> "$tmp_cache"
    mv "$tmp_cache" "$hash_cache"

    echo "$file_hash"
}

# Fast file comparison using hashes
files_identical_hash() {
    local file1="$1"
    local file2="$2"

    # Quick size check first (very fast)
    local size1 size2
    size1=$(stat -f%z "$file1" 2>/dev/null) || return 1
    size2=$(stat -f%z "$file2" 2>/dev/null) || return 1
    [ "$size1" != "$size2" ] && return 1

    # Hash comparison
    local hash1 hash2
    hash1=$(get_file_hash "$file1") || return 1
    hash2=$(get_file_hash "$file2") || return 1
    [ "$hash1" = "$hash2" ]
}
```

Store hash cache at `$BACKUP_DIR/.hash-cache` using pipe-delimited format:
`filepath|mtime|sha256hash`

Avoid: Don't use extended attributes (xattr) - they don't survive cloud sync.
  </action>
  <verify>
1. `get_file_hash somefile` returns 64-char hex string
2. Second call with same mtime returns cached value (faster)
3. `files_identical_hash file1 file2` returns 0 for identical, 1 for different
  </verify>
  <done>
- `get_file_hash()` function works with caching
- `files_identical_hash()` function works
- Hash cache file created at `$BACKUP_DIR/.hash-cache`
  </done>
</task>

<task type="auto">
  <name>Task 2: Replace cmp with hash comparison in backup loop</name>
  <files>bin/backup-now.sh</files>
  <action>
Update the file copy loop in backup-now.sh to use hash comparison:

1. Find the section that uses `cmp -s "$file" "$current_file"`
2. Replace with `files_identical_hash "$file" "$current_file"`
3. Keep `cmp` as fallback if hash functions fail

Pattern:
```bash
# Before copying, check if file actually changed
if [ -f "$current_file" ]; then
    # Try fast hash comparison first
    if files_identical_hash "$file" "$current_file" 2>/dev/null; then
        # Files identical - skip copy
        continue
    fi
fi
# File changed or new - proceed with copy
```

Add config option to disable hash comparison:
```bash
# In backup-config template
BACKUP_USE_HASH_COMPARE="${BACKUP_USE_HASH_COMPARE:-true}"
```

Avoid: Don't remove `cmp` entirely - keep as fallback for edge cases.
  </action>
  <verify>
1. `backup-now --force` uses hash comparison (check with `set -x`)
2. Identical files are skipped (not copied)
3. Changed files are still copied correctly
4. `BACKUP_USE_HASH_COMPARE=false` falls back to cmp
  </verify>
  <done>
- backup-now.sh uses `files_identical_hash()` for comparison
- Config option `BACKUP_USE_HASH_COMPARE` added
- Fallback to `cmp` works when disabled
- Performance improved for large unchanged files
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Create 10MB test file, backup twice - second backup skips unchanged file
- [ ] Modify test file, backup - changed file is copied
- [ ] Hash cache file exists and grows with backups
- [ ] `BACKUP_USE_HASH_COMPARE=false backup-now` uses cmp fallback
- [ ] No bash errors with `set -e` enabled
</verification>

<success_criteria>
- All tasks completed
- Hash-based comparison working
- Hash cache persisted between runs
- Config option to disable
- Measurable performance improvement for unchanged files
</success_criteria>

<output>
After completion, create `.planning/phases/07-performance-optimization/07-02-SUMMARY.md`
</output>
