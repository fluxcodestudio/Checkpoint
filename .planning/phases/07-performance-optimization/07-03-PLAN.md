---
phase: 07-performance-optimization
plan: 03
type: execute
---

<objective>
Consolidate multiple find traversals in cleanup into single-pass operation.

Purpose: Reduce cleanup overhead from 2-10 seconds to <500ms for projects with 1000+ archived files.
Output: Single-traversal cleanup in backup-lib.sh with in-memory filtering.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-performance-optimization/07-01-SUMMARY.md
@.planning/phases/07-performance-optimization/07-02-SUMMARY.md
@.planning/codebase/CONCERNS.md

@lib/backup-lib.sh

**Documented bottleneck (CONCERNS.md lines 46-51):**
> "Multiple Find Operations in Cleanup Analysis: With 1000s of backups, analysis takes minutes. Cause: Sequential directory traversals for expired, duplicate, and orphaned files."

**Current pattern:**
```bash
# Multiple traversals - SLOW
find "$DATABASE_DIR" -name "*.db.gz" -type f -mtime +${DB_RETENTION_DAYS}
find "$DATABASE_DIR" -name "*.db.gz" -type f -mtime +${DB_RETENTION_DAYS} -delete
find "$ARCHIVED_DIR" -type f -mtime +${FILE_RETENTION_DAYS}
find "$ARCHIVED_DIR" -type f -mtime +${FILE_RETENTION_DAYS} -delete
find "$ARCHIVED_DIR" -type d -empty -delete
```

**Solution:** Single traversal with in-memory classification:
1. One `find` command with `-printf` for all metadata
2. Parse output into arrays (expired, orphaned, to-delete)
3. Batch delete operations
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create single-pass cleanup scanner</name>
  <files>lib/backup-lib.sh</files>
  <action>
Add a new function `cleanup_single_pass()` that replaces multiple find operations:

```bash
cleanup_single_pass() {
    local backup_dir="${1:-$BACKUP_DIR}"
    local db_retention="${DB_RETENTION_DAYS:-30}"
    local file_retention="${FILE_RETENTION_DAYS:-7}"
    local archived_dir="${backup_dir}/archived"
    local database_dir="${backup_dir}/databases"

    local expired_dbs=()
    local expired_files=()
    local empty_dirs=()
    local now=$(date +%s)
    local db_cutoff=$((now - db_retention * 86400))
    local file_cutoff=$((now - file_retention * 86400))

    # Single traversal for databases
    if [ -d "$database_dir" ]; then
        while IFS= read -r line; do
            local file mtime
            file="${line%|*}"
            mtime="${line##*|}"
            [ "$mtime" -lt "$db_cutoff" ] && expired_dbs+=("$file")
        done < <(find "$database_dir" -name "*.db.gz" -type f -exec stat -f "%N|%m" {} \; 2>/dev/null)
    fi

    # Single traversal for archived files + empty dirs
    if [ -d "$archived_dir" ]; then
        while IFS= read -r line; do
            local path type mtime
            path="${line%%|*}"
            line="${line#*|}"
            type="${line%%|*}"
            mtime="${line#*|}"

            if [ "$type" = "f" ]; then
                [ "$mtime" -lt "$file_cutoff" ] && expired_files+=("$path")
            elif [ "$type" = "d" ]; then
                # Check if empty
                [ -z "$(ls -A "$path" 2>/dev/null)" ] && empty_dirs+=("$path")
            fi
        done < <(find "$archived_dir" \( -type f -o -type d \) -exec stat -f "%N|%HT|%m" {} \; 2>/dev/null)
    fi

    # Report counts
    echo "Cleanup scan: ${#expired_dbs[@]} expired DBs, ${#expired_files[@]} expired files, ${#empty_dirs[@]} empty dirs"

    # Return arrays via global variables (bash limitation)
    CLEANUP_EXPIRED_DBS=("${expired_dbs[@]}")
    CLEANUP_EXPIRED_FILES=("${expired_files[@]}")
    CLEANUP_EMPTY_DIRS=("${empty_dirs[@]}")
}
```

Add companion function for batch deletion:
```bash
cleanup_execute() {
    local dry_run="${1:-false}"
    local deleted=0

    for f in "${CLEANUP_EXPIRED_DBS[@]}"; do
        [ "$dry_run" = "true" ] && echo "Would delete: $f" && continue
        rm -f "$f" && deleted=$((deleted + 1))
    done

    for f in "${CLEANUP_EXPIRED_FILES[@]}"; do
        [ "$dry_run" = "true" ] && echo "Would delete: $f" && continue
        rm -f "$f" && deleted=$((deleted + 1))
    done

    # Delete empty dirs deepest-first
    for d in $(printf '%s\n' "${CLEANUP_EMPTY_DIRS[@]}" | sort -r); do
        [ "$dry_run" = "true" ] && echo "Would remove dir: $d" && continue
        rmdir "$d" 2>/dev/null && deleted=$((deleted + 1))
    done

    echo "Cleanup complete: $deleted items removed"
}
```

Avoid: Don't use `-printf` (GNU find) - macOS uses BSD find. Use `stat -f` instead.
  </action>
  <verify>
1. `cleanup_single_pass /path/to/backups` completes in <500ms for 1000 files
2. Correct files identified as expired (check CLEANUP_EXPIRED_* arrays)
3. `cleanup_execute true` shows what would be deleted (dry run)
  </verify>
  <done>
- `cleanup_single_pass()` function scans in single traversal
- `cleanup_execute()` function performs batch deletion
- Global arrays hold results for inspection
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate single-pass cleanup into backup workflow</name>
  <files>bin/backup-now.sh, bin/backup-daemon.sh</files>
  <action>
Replace existing cleanup calls with new single-pass functions:

1. Find `generate_cleanup_recommendations()` calls and multi-find cleanup blocks
2. Replace with:
```bash
# Run cleanup if enabled
if [ "${BACKUP_AUTO_CLEANUP:-true}" = "true" ]; then
    cleanup_single_pass "$BACKUP_DIR"
    if [ ${#CLEANUP_EXPIRED_DBS[@]} -gt 0 ] || [ ${#CLEANUP_EXPIRED_FILES[@]} -gt 0 ]; then
        cleanup_execute false
    fi
fi
```

3. Add timing instrumentation to measure improvement:
```bash
if [ "${BACKUP_DEBUG:-false}" = "true" ]; then
    local cleanup_start cleanup_end
    cleanup_start=$(date +%s%3N)
    cleanup_single_pass "$BACKUP_DIR"
    cleanup_execute false
    cleanup_end=$(date +%s%3N)
    log_debug "Cleanup completed in $((cleanup_end - cleanup_start))ms"
fi
```

4. Update `generate_cleanup_recommendations()` to use single-pass data:
```bash
generate_cleanup_recommendations() {
    cleanup_single_pass "$BACKUP_DIR"

    echo "## Cleanup Recommendations"
    echo ""
    echo "Expired databases: ${#CLEANUP_EXPIRED_DBS[@]}"
    echo "Expired archived files: ${#CLEANUP_EXPIRED_FILES[@]}"
    echo "Empty directories: ${#CLEANUP_EMPTY_DIRS[@]}"

    if [ ${#CLEANUP_EXPIRED_DBS[@]} -gt 0 ]; then
        echo ""
        echo "### Expired Database Backups"
        printf '%s\n' "${CLEANUP_EXPIRED_DBS[@]}" | head -10
        [ ${#CLEANUP_EXPIRED_DBS[@]} -gt 10 ] && echo "... and $((${#CLEANUP_EXPIRED_DBS[@]} - 10)) more"
    fi
}
```

Avoid: Don't remove old cleanup functions yet - keep for backward compatibility flag.
  </action>
  <verify>
1. `backup-now --force` uses single-pass cleanup (check with BACKUP_DEBUG=true)
2. Cleanup timing logged when debug enabled
3. `generate_cleanup_recommendations` shows correct counts
4. Old cleanup still available via `BACKUP_USE_LEGACY_CLEANUP=true`
  </verify>
  <done>
- backup-now.sh uses single-pass cleanup
- backup-daemon.sh uses single-pass cleanup
- Debug timing instrumentation added
- Legacy cleanup available as fallback
- Phase 7 complete
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Create test directory with 1000+ archived files
- [ ] `time cleanup_single_pass /test/backup` completes in <500ms
- [ ] `time cleanup_execute true` shows items that would be deleted
- [ ] `backup-now` integrates cleanup without errors
- [ ] BACKUP_DEBUG=true shows cleanup timing
</verification>

<success_criteria>
- All tasks completed
- Single-pass cleanup working
- Measurable performance improvement (10x faster cleanup)
- Debug instrumentation available
- Backward compatibility maintained
- Phase 7: Performance Optimization complete
</success_criteria>

<output>
After completion, create `.planning/phases/07-performance-optimization/07-03-SUMMARY.md`

Final summary should include:
- Total performance improvements achieved
- Metrics: change detection time, file comparison speed, cleanup duration
- Files modified across all 3 plans
</output>
