---
phase: 18-daemon-lifecycle
plan: 02
type: execute
---

<objective>
Add per-project backup staleness detection with tiered notifications and cooldown system.

Purpose: Alert users when backups stop completing (daemon running but backups failing), using escalating severity levels with anti-fatigue cooldowns to prevent notification spam.
Output: Watchdog monitors all registered projects for backup staleness, sends tiered notifications with per-severity per-project cooldowns.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-daemon-lifecycle/18-RESEARCH.md

# Key source files:
@bin/checkpoint-watchdog.sh
@lib/global-status.sh
@lib/projects-registry.sh
@lib/platform/compat.sh

# Prior plan summary:
@.planning/phases/18-daemon-lifecycle/18-01-SUMMARY.md

**Tech stack available:** bash 3.2+, structured logging, cross-platform notifications (send_notification), projects registry (list_projects), health scoring (get_project_health, get_project_backup_age)
**Established patterns:** list_projects iteration (global-status.sh), health thresholds (HEALTH_WARNING_HOURS=24, HEALTH_ERROR_HOURS=72), per-project config overrides (ALERT_WARNING_HOURS, ALERT_ERROR_HOURS)

**From research — key patterns:**
- Tiered staleness: warning (>24h) / critical (>72h) based on existing thresholds
- Notification cooldown: per-severity, per-project state files
- Cooldown periods: warning=4h, critical=2h (escalating urgency = shorter cooldown)
- Consecutive failure threshold: 3 consecutive checks before first alert

**From research — don't hand-roll:**
- Health scoring → reuse existing get_project_health() / get_project_backup_age() from global-status.sh
- Notifications → use existing send_notification() from compat.sh
- Project listing → use existing list_projects() from projects-registry.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add per-project backup staleness detection to watchdog</name>
  <files>bin/checkpoint-watchdog.sh</files>
  <action>
Add backup staleness checking to checkpoint-watchdog.sh. This is SEPARATE from the existing daemon heartbeat monitoring (which checks if the daemon process is alive). Staleness checks whether backups are actually completing.

**1. Source required libraries at the top (after existing sources):**
```bash
# Projects registry and health scoring for staleness detection
source "$SCRIPT_DIR/../lib/projects-registry.sh"
source "$SCRIPT_DIR/../lib/global-status.sh"
```

**2. Add staleness configuration constants (after existing CHECK_INTERVAL):**
```bash
# Staleness notification thresholds (reuse existing health thresholds from global-status.sh)
# HEALTH_WARNING_HOURS (default 24) and HEALTH_ERROR_HOURS (default 72) are loaded from global-status.sh
STALENESS_CHECK_INTERVAL=300  # Check staleness every 5 minutes (not every 60s loop)
```

**3. Add a `check_all_projects_staleness()` function:**
```bash
check_all_projects_staleness() {
    local stale_warning=0
    local stale_error=0
    local stale_projects_warning=""
    local stale_projects_error=""

    while IFS= read -r project_path; do
        [[ -z "$project_path" ]] && continue
        [[ ! -d "$project_path" ]] && continue

        local project_name
        project_name=$(basename "$project_path")
        local health
        health=$(get_project_health "$project_path")

        case "$health" in
            error)
                stale_error=$((stale_error + 1))
                stale_projects_error="${stale_projects_error:+$stale_projects_error, }$project_name"
                ;;
            warning)
                stale_warning=$((stale_warning + 1))
                stale_projects_warning="${stale_projects_warning:+$stale_projects_warning, }$project_name"
                ;;
        esac
    done < <(list_projects)

    # Return results via globals (bash can't return multiple values)
    _STALE_WARNING_COUNT=$stale_warning
    _STALE_ERROR_COUNT=$stale_error
    _STALE_PROJECTS_WARNING="$stale_projects_warning"
    _STALE_PROJECTS_ERROR="$stale_projects_error"
}
```

**4. Add staleness check call in main loop:**

Add a staleness timer and check in the `while true` loop. Use a counter to run staleness check every STALENESS_CHECK_INTERVAL (every 5th iteration at 60s interval):

```bash
local staleness_counter=0
local staleness_check_every=$((STALENESS_CHECK_INTERVAL / CHECK_INTERVAL))
```

Inside the loop, after the heartbeat status case block and before `write_watchdog_heartbeat`:
```bash
# Per-project backup staleness check (every 5 minutes)
staleness_counter=$((staleness_counter + 1))
if [[ $staleness_counter -ge $staleness_check_every ]]; then
    staleness_counter=0
    check_all_projects_staleness

    if [[ $_STALE_ERROR_COUNT -gt 0 ]]; then
        log_warn "Backup staleness CRITICAL: $_STALE_ERROR_COUNT projects: $_STALE_PROJECTS_ERROR"
        if should_notify "global" "critical" "$NOTIFY_COOLDOWN_CRITICAL"; then
            send_notification "Checkpoint Alert" "CRITICAL: $_STALE_ERROR_COUNT project(s) backup overdue (>72h): $_STALE_PROJECTS_ERROR"
        fi
    fi

    if [[ $_STALE_WARNING_COUNT -gt 0 ]]; then
        log_info "Backup staleness WARNING: $_STALE_WARNING_COUNT projects: $_STALE_PROJECTS_WARNING"
        if should_notify "global" "warning" "$NOTIFY_COOLDOWN_WARNING"; then
            send_notification "Checkpoint Warning" "$_STALE_WARNING_COUNT project(s) not backed up recently (>24h): $_STALE_PROJECTS_WARNING"
        fi
    fi

    if [[ $_STALE_ERROR_COUNT -eq 0 ]] && [[ $_STALE_WARNING_COUNT -eq 0 ]]; then
        log_trace "All projects backup status: healthy"
    fi
fi
```

**5. Initialize staleness globals before the loop:**
```bash
_STALE_WARNING_COUNT=0
_STALE_ERROR_COUNT=0
_STALE_PROJECTS_WARNING=""
_STALE_PROJECTS_ERROR=""
```

Do NOT modify the existing heartbeat monitoring logic (read_heartbeat, consecutive_failures, restart logic). Staleness checking is additive — a new feature alongside existing monitoring.
  </action>
  <verify>
1. `bash -n bin/checkpoint-watchdog.sh` passes
2. `grep -c 'check_all_projects_staleness' bin/checkpoint-watchdog.sh` returns at least 2 (definition + call)
3. `grep 'source.*projects-registry' bin/checkpoint-watchdog.sh` shows library sourced
4. `grep 'source.*global-status' bin/checkpoint-watchdog.sh` shows library sourced
5. `grep 'STALENESS_CHECK_INTERVAL' bin/checkpoint-watchdog.sh` shows config constant
  </verify>
  <done>Watchdog checks all registered projects for backup staleness every 5 minutes, using existing health thresholds. Warning and critical staleness detected and logged.</done>
</task>

<task type="auto">
  <name>Task 2: Add notification cooldown system</name>
  <files>bin/checkpoint-watchdog.sh</files>
  <action>
Add a notification cooldown system to prevent alert fatigue. Cooldowns are tracked per-severity using state files.

**1. Add cooldown configuration constants (near other config constants):**
```bash
# Notification cooldown periods (seconds)
NOTIFY_COOLDOWN_WARNING=$((4 * 3600))   # 4 hours between warning notifications
NOTIFY_COOLDOWN_CRITICAL=$((2 * 3600))  # 2 hours between critical notifications
NOTIFY_STATE_DIR="${STATE_DIR}/notify-cooldown"
```

**2. Create `should_notify()` function:**
```bash
# Check if enough time has passed since last notification for this severity
# Args: $1=context (e.g. "global"), $2=severity (warning|critical), $3=cooldown_seconds
# Returns: 0 if should notify, 1 if in cooldown
should_notify() {
    local context="$1"
    local severity="$2"
    local cooldown="$3"
    local state_file="${NOTIFY_STATE_DIR}/${context}-${severity}"

    mkdir -p "$NOTIFY_STATE_DIR"

    local now last elapsed
    now=$(date +%s)
    last=$(cat "$state_file" 2>/dev/null || echo "0")
    elapsed=$((now - last))

    if [[ $elapsed -lt $cooldown ]]; then
        log_trace "Notification suppressed: $context/$severity (cooldown: ${elapsed}s < ${cooldown}s)"
        return 1
    fi

    echo "$now" > "$state_file"
    log_debug "Notification allowed: $context/$severity (elapsed: ${elapsed}s)"
    return 0
}
```

**3. Ensure `should_notify` is defined BEFORE the main loop** (place it with the other helper functions, near write_status / write_watchdog_heartbeat).

**4. Add NOTIFY_STATE_DIR to cleanup function:**
In the existing `cleanup()` function, do NOT remove cooldown state files — they should persist across watchdog restarts to maintain cooldown state.

**5. Ensure `mkdir -p "$NOTIFY_STATE_DIR"` runs at startup** (add near the existing `mkdir -p "$HEARTBEAT_DIR" "${STATE_DIR}/logs"`).

The staleness check code from Task 1 already calls `should_notify()` — this task provides the implementation.

Do NOT add cooldown to the existing daemon restart notification (`send_notification "Checkpoint Watchdog" "Restarted backup daemon after heartbeat timeout"`). Daemon restarts are rare events that should always notify immediately.
  </action>
  <verify>
1. `bash -n bin/checkpoint-watchdog.sh` passes
2. `grep -c 'should_notify' bin/checkpoint-watchdog.sh` returns at least 3 (definition + 2 calls from staleness)
3. `grep 'NOTIFY_COOLDOWN_WARNING' bin/checkpoint-watchdog.sh` shows 4-hour cooldown
4. `grep 'NOTIFY_COOLDOWN_CRITICAL' bin/checkpoint-watchdog.sh` shows 2-hour cooldown
5. `grep 'NOTIFY_STATE_DIR' bin/checkpoint-watchdog.sh` shows state directory for cooldown files
6. `grep 'notify-cooldown' bin/checkpoint-watchdog.sh` shows cooldown state directory name
  </verify>
  <done>Notification cooldown system prevents alert fatigue. Warning notifications suppressed for 4h, critical for 2h. State persists across watchdog restarts. Daemon restart notifications are NOT cooldown-gated.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `bash -n bin/checkpoint-watchdog.sh` passes
- [ ] Staleness check iterates all registered projects
- [ ] Health thresholds reused from global-status.sh (not duplicated)
- [ ] Notifications use existing send_notification() from compat.sh
- [ ] Cooldown state files stored in ~/.checkpoint/notify-cooldown/
- [ ] Cooldown periods: warning=4h, critical=2h
- [ ] Existing heartbeat monitoring and restart logic unchanged
- [ ] Daemon restart notification NOT cooldown-gated
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No syntax errors introduced
- Watchdog detects stale backups per-project and sends tiered notifications
- Notification cooldown prevents alert fatigue
- Existing daemon monitoring functionality preserved
</success_criteria>

<output>
After completion, create `.planning/phases/18-daemon-lifecycle/18-02-SUMMARY.md`
</output>
