---
phase: 04-fallback-chain
plan: 02
type: execute
---

<objective>
Implement local queue for offline scenarios with automatic retry when connectivity restores.

Purpose: Ensure no backup is ever lost — when all remote options fail, queue locally and retry later.
Output: Queue infrastructure with enqueue/dequeue/process functions and automatic retry mechanism.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Prior plan context:**
@.planning/phases/04-fallback-chain/04-01-SUMMARY.md

**Key files:**
@lib/backup-lib.sh - resolve_backup_destinations() with RCLONE_SYNC_PENDING
@lib/cloud-backup.sh - cloud_upload() for rclone sync
@bin/backup-daemon.sh - existing daemon pattern for background processing

**Constraining decisions:**
- 04-01: RCLONE_SYNC_PENDING marks backups needing cloud sync
- Phase 01-02: Silent fallback pattern (don't interrupt user)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create queue infrastructure and enqueue function</name>
  <files>lib/backup-queue.sh</files>
  <action>
Create new lib/backup-queue.sh with queue management functions:

```bash
#!/usr/bin/env bash
# Checkpoint - Backup Queue Library
# Queues failed cloud syncs for retry when connectivity restores

set -euo pipefail

# Queue directory location
BACKUP_QUEUE_DIR="${BACKUP_QUEUE_DIR:-$HOME/.claudecode-backups/queue}"

# Initialize queue directory
init_backup_queue() {
    mkdir -p "$BACKUP_QUEUE_DIR"
}

# Add backup to sync queue
# Args: $1 = project_name, $2 = backup_dir, $3 = sync_type (rclone|cloud_folder)
enqueue_backup_sync() {
    local project_name="$1"
    local backup_dir="$2"
    local sync_type="${3:-rclone}"
    local timestamp=$(date +%s)
    local queue_file="$BACKUP_QUEUE_DIR/${timestamp}_${project_name}.queue"

    init_backup_queue

    # Write queue entry as simple key=value
    cat > "$queue_file" << EOF
PROJECT_NAME=$project_name
BACKUP_DIR=$backup_dir
SYNC_TYPE=$sync_type
QUEUED_AT=$timestamp
RETRY_COUNT=0
EOF

    echo "$queue_file"
}

# List pending queue entries
# Returns: paths to queue files, oldest first
list_queue_entries() {
    init_backup_queue
    find "$BACKUP_QUEUE_DIR" -name "*.queue" -type f 2>/dev/null | sort
}

# Get queue entry count
get_queue_count() {
    list_queue_entries | wc -l | tr -d ' '
}

# Read queue entry into variables
# Args: $1 = queue_file
# Sets: PROJECT_NAME, BACKUP_DIR, SYNC_TYPE, QUEUED_AT, RETRY_COUNT
read_queue_entry() {
    local queue_file="$1"
    if [[ -f "$queue_file" ]]; then
        source "$queue_file"
        return 0
    fi
    return 1
}

# Update retry count in queue entry
# Args: $1 = queue_file
increment_retry_count() {
    local queue_file="$1"
    if [[ -f "$queue_file" ]]; then
        local current_count=$(grep "^RETRY_COUNT=" "$queue_file" | cut -d= -f2)
        local new_count=$((current_count + 1))
        sed -i '' "s/^RETRY_COUNT=.*/RETRY_COUNT=$new_count/" "$queue_file"
    fi
}

# Remove queue entry (after successful sync)
# Args: $1 = queue_file
dequeue_entry() {
    local queue_file="$1"
    rm -f "$queue_file"
}

# Check if queue has entries
has_pending_queue() {
    [[ $(get_queue_count) -gt 0 ]]
}
```

Simple file-based queue:
- One file per queued backup sync
- Timestamp prefix ensures oldest-first processing
- Retry count for exponential backoff
- No complex serialization — source files directly
  </action>
  <verify>bash -n lib/backup-queue.sh passes && grep -c "enqueue_backup_sync\|dequeue_entry" lib/backup-queue.sh shows 2+ matches</verify>
  <done>lib/backup-queue.sh exists with enqueue_backup_sync(), list_queue_entries(), dequeue_entry() functions</done>
</task>

<task type="auto">
  <name>Task 2: Add queue processor with retry on connectivity</name>
  <files>lib/backup-queue.sh, bin/backup-now.sh</files>
  <action>
1. Add process_backup_queue() to lib/backup-queue.sh:

```bash
# Process pending queue entries
# Attempts sync for each entry, removes on success, increments retry on failure
# Args: $1 = max_entries (optional, default 10)
process_backup_queue() {
    local max_entries="${1:-10}"
    local processed=0
    local succeeded=0
    local failed=0

    # Load cloud-backup for rclone functions
    local lib_dir
    lib_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    source "$lib_dir/cloud-backup.sh" 2>/dev/null || true

    while IFS= read -r queue_file; do
        [[ -z "$queue_file" ]] && continue
        ((processed >= max_entries)) && break

        # Load entry variables
        if ! read_queue_entry "$queue_file"; then
            dequeue_entry "$queue_file"  # Corrupt entry, remove
            continue
        fi

        local sync_success=false

        # Try rclone sync
        if [[ "$SYNC_TYPE" == "rclone" ]] && [[ "${CLOUD_ENABLED:-false}" == "true" ]]; then
            # Set required vars for cloud_upload
            export LOCAL_BACKUP_DIR="$BACKUP_DIR"

            if cloud_upload 2>/dev/null; then
                sync_success=true
            fi
        fi

        if $sync_success; then
            dequeue_entry "$queue_file"
            ((succeeded++))
        else
            increment_retry_count "$queue_file"
            ((failed++))

            # Max 5 retries, then give up (but keep in queue for manual review)
            local retry_count=$(grep "^RETRY_COUNT=" "$queue_file" | cut -d= -f2)
            if [[ $retry_count -ge 5 ]]; then
                mv "$queue_file" "${queue_file}.failed"
            fi
        fi

        ((processed++))
    done < <(list_queue_entries)

    echo "Queue processed: $processed entries, $succeeded synced, $failed failed"
}
```

2. Update bin/backup-now.sh to enqueue when rclone fallback is used. After backup completes, add:

```bash
# Near end of backup-now.sh, after successful backup
# Queue for cloud sync if rclone fallback was triggered
if [[ "${RCLONE_SYNC_PENDING:-false}" == "true" ]]; then
    source "$LIB_DIR/backup-queue.sh"
    enqueue_backup_sync "$PROJECT_NAME" "$PRIMARY_BACKUP_DIR" "rclone"
    backup_log "Backup queued for cloud sync when connectivity restores" "INFO"
fi

# Opportunistically process queue (non-blocking, max 3 entries)
if has_pending_queue 2>/dev/null; then
    process_backup_queue 3 &
fi
```

The pattern:
- Backup completes locally → if RCLONE_SYNC_PENDING, add to queue
- Each backup run opportunistically processes queue entries
- Failed syncs stay in queue, retried with exponential backoff
- After 5 failures, moved to .failed for manual review
  </action>
  <verify>grep -A5 "RCLONE_SYNC_PENDING" bin/backup-now.sh shows enqueue logic && grep -c "process_backup_queue" lib/backup-queue.sh shows 1+ match</verify>
  <done>Backups queue when rclone fallback used; queue processor retries on each backup run</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] lib/backup-queue.sh exists with queue management functions
- [ ] enqueue_backup_sync() creates queue entry file
- [ ] process_backup_queue() processes entries and removes on success
- [ ] bin/backup-now.sh enqueues when RCLONE_SYNC_PENDING=true
- [ ] No syntax errors: bash -n lib/backup-queue.sh && bash -n bin/backup-now.sh
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Queue files created in ~/.claudecode-backups/queue/
- Failed syncs automatically retry on subsequent backup runs
- Phase 4 complete
</success_criteria>

<output>
After completion, create `.planning/phases/04-fallback-chain/04-02-SUMMARY.md`

Update STATE.md:
- Phase: 4 complete
- Next: Phase 5 (Tiered Retention)
</output>
